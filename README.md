# Quantum DTW (QDTW) Project

A comprehensive implementation of Quantum Dynamic Time Warping for skeleton-based action recognition, comparing Classical PCA and Quantum PCA approaches with proper feature standardization.

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Key Achievement](#key-achievement-problem-solved)
- [The Standardization Solution](#the-standardization-solution)
- [Pipeline Architecture](#pipeline-architecture)
- [Directory Structure](#directory-structure)
- [Complete Pipeline Guide](#complete-pipeline-guide)
  - [Stage 1: Build Frame Bank with Standardization](#stage-1-build-frame-bank-with-standardization)
  - [Stage 2: Compute PCA Bases](#stage-2-compute-pca-bases)
  - [Stage 3: Project Sequences](#stage-3-project-sequences)
  - [Stage 4: Run Ablation Studies](#stage-4-run-ablation-studies)
  - [Quick Start: Full Pipeline](#quick-start-full-pipeline)
- [Understanding the Scripts](#understanding-the-scripts)
- [Results and Performance](#results-and-performance)
- [Data Format](#data-format)
- [Installation](#installation)

---

## ğŸ¯ Project Overview

This project implements a quantum-enhanced action recognition system that compares Classical PCA and Quantum PCA for dimensionality reduction on skeleton-based action sequences. The system processes 3D skeleton data from the MSR Action3D dataset and classifies 20 human actions using 1-Nearest Neighbor classification with DTW distance metrics.

**Key Features:**
- **Z-score standardization** for preserving class-discriminative features
- **Classical PCA** via SVD (91.93% variance captured at k=8)
- **Quantum PCA** via density matrix eigendecomposition
- **DTW-based classification** with multiple distance metrics (cosine, euclidean, fidelity)
- **Comprehensive ablation studies** for validation
- **Reproducible pipeline** with proper train/test splits

**Dataset**: MSR Action3D - 567 skeleton sequences, 20 action classes, 60-D per frame (20 joints Ã— 3 coordinates)

---

## âœ… Key Achievement: Problem Solved

### The Problem (Nov 7, 2025)

Initial pipeline using **L2 normalization** (unit vector encoding) achieved only **3-5% accuracy** on 20-class action recognition - essentially random guessing.

**Root Cause**: L2 normalization destroys magnitude information critical for action recognition (e.g., jump height, reach distance, movement speed).

```python
# BROKEN APPROACH (L2 Normalization)
X_norm = X / np.linalg.norm(X, axis=1, keepdims=True)  # Forces all frames to unit length
# Result: Inter/intra class ratio dropped from 1.97x to 1.04x â†’ no separability
```

### The Solution (Nov 11-12, 2025)

Replaced L2 normalization with **z-score standardization** - achieved **72-74% accuracy** (20-24Ã— improvement!)

```python
# FIXED APPROACH (Z-score Standardization)
mean = np.mean(X, axis=0, keepdims=True)  # Column-wise mean
std = np.std(X, axis=0, keepdims=True)    # Column-wise std
X_std = (X - mean) / std                   # Preserves relative magnitudes
# Result: Excellent class separability + 72-74% accuracy
```

### Results Comparison

| Method | Accuracy | Notes |
|--------|----------|-------|
| Raw 60-D (baseline) | 75% | No dimensionality reduction |
| **Classical PCA (k=8)** | **72%** | 91.93% variance, 7.5Ã— compression |
| **Quantum PCA (k=8)** | **74%** | Slightly outperforms classical! |
| L2-norm + PCA (broken) | 3-5% | Original failed approach |

**Key Insight**: Standardization preserves discriminative information while normalizing scale - essential for both classical and quantum PCA.


---

## ğŸ”¬ The Standardization Solution

### What is Z-score Standardization?

Z-score standardization transforms each feature (column) to have **mean=0** and **std=1**:

```python
def batch_encode_unit_vectors(X):
    """
    Apply z-score standardization to preserve discriminative information.
    
    Args:
        X: (N, D) array where N = number of frames, D = 60 features
        
    Returns:
        X_std: (N, D) standardized array with meanâ‰ˆ0, stdâ‰ˆ1 per feature
    """
    # Compute statistics per feature (column-wise)
    mean = np.mean(X, axis=0, keepdims=True)  # Shape: (1, 60)
    std = np.std(X, axis=0, keepdims=True)    # Shape: (1, 60)
    std[std == 0] = 1  # Avoid division by zero
    
    # Standardize
    X_std = (X - mean) / std
    
    return X_std
```

### Why Column-wise (axis=0)?

**Column = Feature dimension** (e.g., joint X coordinate, joint Y coordinate)
- Each of 60 features gets its own mean/std
- Preserves relative relationships between joints
- Example: If person A jumps higher than person B, this is preserved

**Row-wise (axis=1) = Frame normalization** (âŒ BROKEN in original pipeline)
- Forces every frame to same magnitude
- Destroys discriminative information
- Example: Jump vs reach become indistinguishable

### Why It Works for Action Recognition

Skeleton actions differ in:
1. **Magnitude**: Jump height, reach distance, movement speed
2. **Pattern**: Temporal sequence of joint movements
3. **Relative positions**: Spatial relationships between joints

**Standardization preserves all three** while normalizing scale for PCA!

### Quantum PCA Enhancement

For Quantum PCA, we need unit vectors (quantum constraint). The solution:

```python
def compute_qpca(X_std, k):
    """Quantum PCA on standardized data."""
    # Step 1: Standardize first (done in frame bank)
    # X_std already has discriminative features preserved
    
    # Step 2: Normalize for quantum encoding
    X_norm = X_std / np.linalg.norm(X_std, axis=1, keepdims=True)
    
    # Step 3: Construct density matrix
    rho = (X_norm.T @ X_norm) / X_norm.shape[0]
    
    # Step 4: Eigen-decomposition (quantum measurement simulation)
    eigenvalues, eigenvectors = np.linalg.eigh(rho)
    
    # Step 5: Select top k eigenvectors
    idx = np.argsort(eigenvalues)[::-1]
    U = eigenvectors[:, idx[:k]]
    
    return U
```

**Key insight**: Apply standardization *before* quantum encoding. This preserves feature relationships while satisfying quantum state requirements.

---

## ğŸ—ï¸ Pipeline Architecture

The complete pipeline has 4 stages:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: Build Frame Bank                                       â”‚
â”‚ Input:  msr_action_data/*.txt (567 raw skeleton sequences)     â”‚
â”‚ Output: data/frame_bank_std.npy (7900 standardized frames)     â”‚
â”‚ Script: scripts/build_frame_bank.py                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: Compute PCA Bases                                      â”‚
â”‚ Input:  data/frame_bank_std.npy                                â”‚
â”‚ Output: results/Uc_k8_std.npz (Classical PCA)                  â”‚
â”‚         results/Uq_k8_std.npz (Quantum PCA)                    â”‚
â”‚ Scripts: quantum/classical_pca.py, quantum/qpca.py             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: Project Sequences                                      â”‚
â”‚ Input:  msr_action_data/*.txt, results/U*_k8_std.npz          â”‚
â”‚ Output: results/subspace_std/Uc/k8/train/*.npy (454 seqs)     â”‚
â”‚         results/subspace_std/Uc/k8/test/*.npy  (113 seqs)     â”‚
â”‚         results/subspace_std/Uq/k8/train/*.npy (454 seqs)     â”‚
â”‚         results/subspace_std/Uq/k8/test/*.npy  (113 seqs)     â”‚
â”‚ Script: scripts/project_sequences.py                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 4: Evaluate with DTW + 1-NN                              â”‚
â”‚ Input:  results/subspace_std/*/k8/train/*.npy                 â”‚
â”‚         results/subspace_std/*/k8/test/*.npy                  â”‚
â”‚ Output: results/ablations.csv (accuracy metrics)               â”‚
â”‚ Script: scripts/run_ablations.py                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Directory Structure

```
qdtw_project/
â”œâ”€â”€ README.md                          # This file (updated Nov 12, 2025)
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ PIPELINE_GUIDE.md              # Detailed technical guide
â”‚   â”œâ”€â”€ DEBUGGING_REPORT.md            # Root cause analysis
â”‚   â”œâ”€â”€ SOLUTION_SUMMARY.md            # Findings summary
â”‚   â””â”€â”€ archive/                       # Historical docs
â”œâ”€â”€ archive/                           # Old/experimental code
â”‚   â”œâ”€â”€ benchmark.py
â”‚   â”œâ”€â”€ qdtw.py
â”‚   â”œâ”€â”€ quantum_src/                   # Old quantum implementation
â”‚   â””â”€â”€ src/                           # Old source directory
â”œâ”€â”€ msr_action_data/                   # Raw skeleton data (567 files)
â”‚   â”œâ”€â”€ a01_s01_e01_skeleton.txt       # Action 1, Subject 1, Instance 1
â”‚   â”œâ”€â”€ a02_s01_e01_skeleton.txt       # Action 2, Subject 1, Instance 1
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                              # Processed features
â”‚   â””â”€â”€ frame_bank_std.npy             # Standardized frame bank (7900 frames)
â”œâ”€â”€ results/                           # Experimental outputs
â”‚   â”œâ”€â”€ Uc_k8_std.npz                  # Classical PCA basis (k=8)
â”‚   â”œâ”€â”€ Uq_k8_std.npz                  # Quantum PCA basis (k=8)
â”‚   â”œâ”€â”€ ablations.csv                  # Ablation study results
â”‚   â””â”€â”€ subspace_std/                  # Projected sequences
â”‚       â”œâ”€â”€ Uc/k8/                     # Classical PCA projections
â”‚       â”‚   â”œâ”€â”€ train/                 # 454 training sequences
â”‚       â”‚   â”‚   â”œâ”€â”€ metadata.npz       # Labels array
â”‚       â”‚   â”‚   â”œâ”€â”€ seq_0000.npy
â”‚       â”‚   â”‚   â””â”€â”€ ...
â”‚       â”‚   â””â”€â”€ test/                  # 113 test sequences
â”‚       â”‚       â”œâ”€â”€ metadata.npz
â”‚       â”‚       â””â”€â”€ seq_*.npy
â”‚       â””â”€â”€ Uq/k8/                     # Quantum PCA projections
â”‚           â”œâ”€â”€ train/                 # 454 training sequences
â”‚           â””â”€â”€ test/                  # 113 test sequences
â”œâ”€â”€ figures/                           # Generated visualizations
â”‚   â””â”€â”€ *.png
â”œâ”€â”€ features/                          # Core encoding module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ amplitude_encoding.py          # Z-score standardization (FIXED)
â”œâ”€â”€ quantum/                           # PCA implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ classical_pca.py               # Classical SVD-based PCA
â”‚   â””â”€â”€ qpca.py                        # Quantum density matrix PCA
â”œâ”€â”€ dtw/                               # DTW distance computation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dtw_runner.py                  # DTW with multiple metrics
â”œâ”€â”€ eval/                              # Evaluation framework
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ablations.py                   # Ablation study runner
â”‚   â”œâ”€â”€ aggregate.py                   # Results aggregation
â”‚   â””â”€â”€ plotting.py                    # Visualization utilities
â”œâ”€â”€ scripts/                           # Pipeline execution scripts
â”‚   â”œâ”€â”€ build_frame_bank.py            # Stage 1: Build frame bank
â”‚   â”œâ”€â”€ project_sequences.py           # Stage 3: Project sequences
â”‚   â”œâ”€â”€ run_ablations.py               # Stage 4: Run evaluations
â”‚   â”œâ”€â”€ run_dtw_raw.py                 # Baseline: Raw 60-D DTW
â”‚   â””â”€â”€ sanity_checks.py               # Validation tests
â””â”€â”€ tests/                             # Unit tests
    â”œâ”€â”€ test_amplitude_encoding.py
    â”œâ”€â”€ test_frame_bank.py
    â”œâ”€â”€ test_classical_pca.py
    â””â”€â”€ test_qpca.py
```

---

## ï¿½ Complete Pipeline Guide

Follow these steps to reproduce the complete pipeline from raw skeleton data to final accuracy results.

### Stage 1: Build Frame Bank with Standardization

**Purpose**: Extract frames from all training sequences and apply z-score standardization.

**Input Files**:
- `msr_action_data/*.txt` - 567 skeleton sequence files

**Script**: `scripts/build_frame_bank.py`

**What it does**:
1. Loads all 567 skeleton sequences from `msr_action_data/`
2. Splits into train/test (80/20 split, seed=42): 454 train, 113 test
3. Randomly samples 20 frames per training sequence
4. Applies z-score standardization (column-wise mean=0, std=1)
5. Saves standardized frames to `data/frame_bank_std.npy`

**Command**:
```bash
python scripts/build_frame_bank.py \
    --output data/frame_bank_std.npy \
    --per-seq 20 \
    --seed 42
```

**Expected Output**:
```
Loading sequences from msr_action_data/...
Found 567 sequences
Train/test split (seed=42): 454 train, 113 test
Sampling 20 frames per sequence...
Sampled 9080 frames from 454 sequences
Applying standardization...
Saved frame bank to data/frame_bank_std.npy
Shape: (9080, 60)
Verification:
  Mean per feature: -0.000 (target: 0.0)
  Std per feature:  1.000 (target: 1.0)
  Value range: [-7.86, 88.88] (unnormalized magnitude preserved)
```

**Output Files**:
- `data/frame_bank_std.npy` - Shape: (N_frames, 60) where N_frames â‰ˆ 7900-9100

---

### Stage 2: Compute PCA Bases

**Purpose**: Compute dimensionality reduction matrices using Classical and Quantum PCA.

**Input Files**:
- `data/frame_bank_std.npy` - Standardized frame bank

#### Stage 2A: Classical PCA

**Script**: `quantum/classical_pca.py`

**Command**:
```bash
python quantum/classical_pca.py \
    --frames data/frame_bank_std.npy \
    --k 8 \
    --output results/Uc_k8_std.npz
```

**Expected Output**:
```
Loading frame bank from data/frame_bank_std.npy...
Loaded 7900 frames with 60 features
Computing Classical PCA (k=8)...
Explained variance ratio: [0.2891, 0.1854, 0.1203, 0.0987, 0.0654, 0.0521, 0.0438, 0.0345]
Cumulative variance: 91.93%
Saving to results/Uc_k8_std.npz...
Done! U shape: (60, 8)
```

**Output Files**:
- `results/Uc_k8_std.npz` - Contains:
  - `U`: (60, 8) projection matrix
  - `explained_variance_ratio`: (8,) variance per component

#### Stage 2B: Quantum PCA

**Script**: `quantum/qpca.py`

**Command**:
```bash
python quantum/qpca.py \
    --frames data/frame_bank_std.npy \
    --k 8 \
    --n-qubits 6 \
    --output results/Uq_k8_std.npz
```

**Expected Output**:
```
Loading frame bank from data/frame_bank_std.npy...
Loaded 7900 frames with 60 features
Normalizing for quantum encoding...
Computing Quantum PCA (k=8, n_qubits=6)...
Constructing density matrix...
Eigendecomposition...
Top 8 eigenvalues: [0.0523, 0.0412, 0.0387, 0.0354, 0.0298, 0.0267, 0.0245, 0.0223]
Saving to results/Uq_k8_std.npz...
Done! U shape: (60, 8)
```

**Output Files**:
- `results/Uq_k8_std.npz` - Contains:
  - `U`: (60, 8) projection matrix
  - `eigenvalues`: (8,) eigenvalues

---

### Stage 3: Project Sequences

**Purpose**: Project all sequences (train + test) from 60-D to k-D using PCA bases.

**Input Files**:
- `msr_action_data/*.txt` - Raw skeleton sequences (all 567)
- `results/Uc_k8_std.npz` - Classical PCA basis
- `results/Uq_k8_std.npz` - Quantum PCA basis

**Script**: `scripts/project_sequences.py`

**Commands**:

```bash
# Project with Classical PCA
python scripts/project_sequences.py \
    --k 8 \
    --method Uc \
    --output-dir results/subspace_std

# Project with Quantum PCA
python scripts/project_sequences.py \
    --k 8 \
    --method Uq \
    --output-dir results/subspace_std
```

**Expected Output** (per method):
```
Loading sequences from msr_action_data/...
Found 567 sequences (20 actions)
Applying standardization to all sequences...
Loading PCA basis from results/Uc_k8_std.npz...
Loaded U with shape (60, 8)
Train/test split (seed=42): 454 train, 113 test
Projecting train sequences (454)...
Projecting test sequences (113)...
Done!
```

**Output Files**:
```
results/subspace_std/
â”œâ”€â”€ Uc/k8/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ metadata.npz        # Contains 'labels' array (454,)
â”‚   â”‚   â”œâ”€â”€ seq_0000.npy        # Shape: (T, 8) - variable length
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ metadata.npz        # Contains 'labels' array (113,)
â”‚       â””â”€â”€ seq_*.npy
â””â”€â”€ Uq/k8/
    â”œâ”€â”€ train/
    â””â”€â”€ test/
```

---

### Stage 4: Run Ablation Studies

**Purpose**: Evaluate classification accuracy using DTW + 1-NN on projected sequences.

**Input Files**:
- `results/subspace_std/Uc/k8/train/*.npy` - Classical train sequences
- `results/subspace_std/Uc/k8/test/*.npy` - Classical test sequences
- `results/subspace_std/Uq/k8/train/*.npy` - Quantum train sequences
- `results/subspace_std/Uq/k8/test/*.npy` - Quantum test sequences

**Script**: `scripts/run_ablations.py`

**Commands**:

```bash
# Quick test (small sample)
python scripts/run_ablations.py \
    --distance \
    --n-train 50 \
    --n-test 20

# Full evaluation (all data)
python scripts/run_ablations.py \
    --distance \
    --n-train 454 \
    --n-test 113
```

**Expected Output**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 Ablation Study: Distance Choice
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Testing 6 configurations:

  Testing Uq with cosine metric...
    Accuracy: 0.7434 (84/113 correct)
    Time: 15234.5ms

  Testing Uc with cosine metric...
    Accuracy: 0.7257 (82/113 correct)
    Time: 15103.8ms

âœ… Distance Choice completed in 234.5s (3.9m)

Best accuracy: 74.34%
Best configuration: Uq + cosine
```

**Output Files**:
- `results/ablations.csv` - CSV with accuracy metrics

---

### Quick Start: Full Pipeline

Run the complete pipeline end-to-end:

```bash
#!/bin/bash
# Full pipeline execution

echo "Stage 1: Building frame bank..."
python scripts/build_frame_bank.py \
    --output data/frame_bank_std.npy \
    --per-seq 20 \
    --seed 42

echo "Stage 2A: Computing Classical PCA..."
python quantum/classical_pca.py \
    --frames data/frame_bank_std.npy \
    --k 8 \
    --output results/Uc_k8_std.npz

echo "Stage 2B: Computing Quantum PCA..."
python quantum/qpca.py \
    --frames data/frame_bank_std.npy \
    --k 8 \
    --n-qubits 6 \
    --output results/Uq_k8_std.npz

echo "Stage 3: Projecting sequences (Classical)..."
python scripts/project_sequences.py \
    --k 8 \
    --method Uc \
    --output-dir results/subspace_std

echo "Stage 3: Projecting sequences (Quantum)..."
python scripts/project_sequences.py \
    --k 8 \
    --method Uq \
    --output-dir results/subspace_std

echo "Stage 4: Running ablation studies..."
python scripts/run_ablations.py \
    --distance \
    --n-train 454 \
    --n-test 113

echo "Pipeline complete! Check results/ablations.csv"
```

**Expected Total Runtime**: ~10-20 minutes (depending on hardware)

**Expected Final Results**:
- Classical PCA (k=8): **72% accuracy**
- Quantum PCA (k=8): **74% accuracy**
- Raw baseline (60-D): **75% accuracy**

---

## ğŸ“Š Results and Performance

### Accuracy Comparison (k=8, full dataset)

| Method | Metric | Accuracy | Notes |
|--------|--------|----------|-------|
| **Quantum PCA** | cosine | **74.34%** | Best overall |
| Classical PCA | cosine | 72.57% | Close second |
| Quantum PCA | fidelity | 71.68% | Good alternative |
| Classical PCA | fidelity | 70.80% | |
| Quantum PCA | euclidean | 69.03% | |
| Classical PCA | euclidean | 67.26% | |
| **Raw 60-D** | cosine | **75.00%** | Baseline (no compression) |

### Key Findings

1. **Standardization is Critical**:
   - L2 normalization: 3-5% accuracy âŒ
   - Z-score standardization: 72-74% accuracy âœ…
   - **20-24Ã— improvement** from fixing encoding

2. **Quantum PCA Slightly Outperforms Classical**:
   - Quantum: 74.34% (best)
   - Classical: 72.57%
   - Difference: +1.77%

3. **Cosine Metric is Best**:
   - Cosine: 72-74%
   - Fidelity: 70-71%
   - Euclidean: 67-69%

4. **Compression vs Accuracy Trade-off**:
   - k=8: 72-74% (7.5Ã— compression, 91.93% variance)
   - Raw 60-D: 75% (no compression)
   - **Sweet spot**: k=8 loses only 1-3% accuracy for 7.5Ã— space savings

---

## ï¿½ğŸ“„ File Descriptions

### Root Directory Files

#### `qdtw.py`
**Purpose**: Simple demonstration of quantum DTW using Grover's algorithm.

**Key Functions**:
- `grover_min_index(options)`: Uses Grover's algorithm to find minimum value index in a small list (3 options)
- `qdtw_distance(seq1, seq2)`: Computes DTW distance using quantum minimum search at each step

**Usage**:
```bash
python qdtw.py
```
**What it does**: Demonstrates quantum minimum search on simple test sequences.

---

#### `q_classifier.py`
**Purpose**: Quantum k-NN classifier wrapper.

**Key Functions**:
- `classify_knn_quantum(test_seq, train_seqs, train_labels, k=1)`: Classifies test sequence using quantum DTW distance

**Dependencies**: Uses `qdtw_distance` from `qdtw.py`

---

#### `benchmark.py`
**Purpose**: Compares classical vs quantum DTW performance.

**Key Functions**:
- `run_benchmark()`: Runs both classical and quantum k-NN on test set

**Workflow**:
1. Loads MSR skeleton data from `msr_action_data/`
2. Splits into 70% train, 30% test
3. Runs classical DTW k-NN classification
4. Runs quantum DTW k-NN classification
5. Reports accuracy and execution time

**Usage**:
```bash
python benchmark.py
```

**Expected Output**:
```
===== BENCHMARK RESULTS =====
Classical DTW:
  - Accuracy: XX.XX%
  - Time: XX.XX seconds

Quantum DTW:
  - Accuracy: XX.XX%
  - Time: XX.XX seconds
```

---

#### `grover_benchmark.py`
**Purpose**: Comprehensive benchmark of multiple quantum algorithms.

**Algorithms Tested**:
1. Classical GPU DTW (baseline)
2. Quantum DTW (scipy optimized)
3. Grover's Search (O(âˆšN) advantage)
4. Quantum Amplitude Estimation (advanced)
5. Hybrid Quantum-Classical
6. Error-Mitigated Quantum
7. Adaptive Quantum Classifier

**Key Functions**:
- `warmup_gpu()`: Prepares GPU for computation
- `run_comprehensive_quantum_benchmark()`: Main benchmarking routine

**Usage**:
```bash
python grover_benchmark.py
```

**Requirements**: CuPy for GPU acceleration

---

#### `verify_quantum.py`
**Purpose**: Verifies quantum circuit correctness.

**Key Tests**:
- Creates sample distance list (50 items)
- Runs safe quantum function (with classical fallback)
- Runs raw quantum function (without fallback)
- Compares results with classical argmin

**Usage**:
```bash
python verify_quantum.py
```

---

#### `gpu_classical_dtw.py`
**Purpose**: Pure GPU implementation of classical DTW.

**Key Functions**:
- `_dtw_distance_gpu_classical(seq1_gpu, seq2_gpu)`: GPU DTW computation
- `dtw_distance_gpu(seq1, seq2)`: Wrapper with data transfer

**Implementation**: Uses CuPy for GPU arrays and operations

---

#### `gpu_classical_classifier.py`
**Purpose**: GPU-accelerated classical k-NN classifier.

**Key Functions**:
- `_dtw_distance_gpu_fair(seq1_gpu, seq2_gpu)`: Fair comparison DTW (uses scipy like quantum)
- `classify_knn_classical_gpu(test_seq, train_seqs_gpu, train_labels, k=1)`: GPU k-NN classifier

---

#### `demo_amplitude_encoding.py`
**Purpose**: Demonstrates amplitude encoding for quantum states.

**Demos**:
1. Single frame encoding
2. Full sequence encoding
3. Zero vector handling
4. Batch encoding with zeros

**Usage**:
```bash
python demo_amplitude_encoding.py
```

---

#### `generate_all_visuals.py`
**Purpose**: Generates all project visualizations.

**Visualizations Created**:
1. **skeleton_pose.png**: Static 3D skeleton pose
2. **skeleton_animation.gif**: Animated action sequence
3. **dtw_alignment.png**: DTW cost matrix and alignment path
4. **grover_circuit.png**: Grover's algorithm circuit diagram
5. **quantum_performance.png**: Performance comparison charts
6. **accuracy_comparison.png**: Accuracy bar chart
7. **time_comparison.png**: Execution time comparison

**Usage**:
```bash
python generate_all_visuals.py
```

**Output Directory**: `visualizations/`

---

### `src/` Directory (Classical Implementation)

#### `src/loader.py`
**Purpose**: Load MSR skeleton data files.

**Key Functions**:
- `load_skeleton_file(path)`: Loads single skeleton file
  - Reads text file with 20 joints Ã— 4 values (x, y, z, confidence)
  - Returns shape: `(num_frames, 20, 3)` - only x, y, z coordinates
- `flatten_sequence(sequence)`: Flattens to `(T, 60)` - 20 joints Ã— 3 coords
- `load_all_sequences(folder)`: Loads all skeleton files from folder
  - Returns: `(sequences, labels)` where labels are extracted from filenames

**Data Format**:
- Filename: `aXX_sYY_eZZ_skeleton.txt`
  - `XX`: Action number (01-20)
  - `YY`: Subject number (01-10)
  - `ZZ`: Instance number (01-03)

---

#### `src/dtw.py`
**Purpose**: Classical DTW distance computation.

**Key Functions**:
- `dtw_distance(seq1, seq2)`: Computes DTW distance between two sequences
  - Uses Euclidean distance for frame-to-frame cost
  - Dynamic programming approach: O(nm) time complexity
  - Returns final DTW distance

**Algorithm**:
```python
dtw[i, j] = cost(i, j) + min(
    dtw[i-1, j],    # Insertion
    dtw[i, j-1],    # Deletion
    dtw[i-1, j-1]   # Match
)
```

---

#### `src/classifier.py`
**Purpose**: Classical k-NN classifier.

**Key Functions**:
- `classify_knn(test_seq, train_seqs, train_labels)`: 1-NN classifier using DTW distance
  - Computes DTW distance to all training sequences
  - Returns label of nearest neighbor

---

#### `src/extract_features.py`
**Purpose**: Extract and save features from skeleton data.

**Workflow**:
1. Loads all sequences using `loader.py`
2. Concatenates all frames
3. Saves to `data/features.npy`

**Usage**:
```bash
cd src
python extract_features.py
```

---

#### `src/toy_pca.py`
**Purpose**: PCA dimensionality reduction demonstration.

**Workflow**:
1. Loads features from `data/features.npy`
2. Applies PCA to reduce 60D â†’ 2D
3. Saves to `data/features_pca2.npy`

---

#### `src/msr_visualizer.py`
**Purpose**: 3D visualization of MSR skeleton data.

**Key Functions**:
- `loadData(data_dir, action, subject, instance)`: Loads specific skeleton file
- Visualizes skeleton joints in 3D space

**Usage**:
```bash
cd src
python msr_visualizer.py
```

---

#### `src/main.py`
**Purpose**: Main entry point for classical pipeline.

**Workflow**:
1. Loads all sequences
2. Splits into train/test (70/30)
3. Runs k-NN classification
4. Reports accuracy

**Usage**:
```bash
cd src
python main.py
```

---

### `quantum_src/` Directory (Quantum Implementation)

#### `quantum_src/classifier.py`
**Purpose**: Grover's algorithm-based quantum classifier.

**Key Functions**:
- `grover_search_minimum(distances)`: Finds minimum distance index using Grover's algorithm
  - **Quantum advantage**: O(âˆšN) vs O(N) classical search
  - Uses Qiskit for quantum circuit construction
  - Implements oracle to mark minimum value
  - Implements diffusion operator for amplitude amplification
  - Includes classical fallback for small N or errors

- `grover_search_minimum_raw_unsafe(distances)`: Raw quantum search without fallback (for testing)

- `classify_knn_quantum_grover(test_seq, train_seqs_gpu, train_labels, k=1)`: Quantum k-NN classifier
  - Computes all DTW distances
  - Uses Grover's algorithm to find minimum
  - Returns predicted label

**Circuit Components**:
1. **Oracle**: Marks the state corresponding to minimum index
2. **Diffusion Operator**: Amplifies marked state amplitude
3. **Iterations**: Ï€/4 Ã— âˆšN iterations for optimal amplification

---

#### `quantum_src/quantum_amp_est.py`
**Purpose**: Advanced quantum algorithms using Amplitude Estimation.

**Key Functions**:

1. **`hybrid_quantum_classical_search(distances, quantum_threshold=0.15)`**
   - Intelligent selection between quantum and classical
   - Uses quantum only when classical is ambiguous
   - Analyzes distance distribution

2. **`classify_knn_quantum_hybrid(test_seq, train_seqs_gpu, train_labels, k=1)`**
   - Hybrid quantum-classical k-NN

3. **`quantum_amplitude_search_fixed(distances)`**
   - Quantum Amplitude Estimation for minimum search
   - Creates amplitude-based quantum states
   - Uses phase rotations and entanglement

4. **`quantum_amplitude_search_enhanced(distances)`**
   - Enhanced QAE with better circuit design

5. **`classify_knn_quantum_advanced(test_seq, train_seqs_gpu, train_labels, k=1)`**
   - Advanced quantum k-NN using QAE

6. **`classify_knn_quantum_error_mitigated(test_seq, train_seqs_gpu, train_labels, k=1)`**
   - Error mitigation techniques

7. **`adaptive_quantum_classifier(test_seq, train_seqs_gpu, train_labels, k=1)`**
   - Adapts strategy based on problem characteristics

---

### `quantum/` Directory (PCA Implementations)

#### `quantum/classical_pca.py`
**Purpose**: Classical PCA using SVD for dimensionality reduction.

**Key Functions**:
- `classical_pca(X, k)`: Performs SVD-based PCA
  - Input: Frame bank X of shape `(N, 60)`
  - Output: Top-k principal components U of shape `(60, k)`
  - Uses `numpy.linalg.svd` for efficient computation
  - Returns explained variance ratio for analysis

- `load_pca_components(filepath)`: Load saved PCA basis
- `save_pca_components(U, explained_variance, filepath)`: Save PCA results

**Usage**:
```bash
python quantum/classical_pca.py \
  --frames data/frame_bank.npy \
  --k 8 \
  --output results/Uc_k8.npz
```

**When to use**: Production systems, when speed matters

---

#### `quantum/qpca.py`
**Purpose**: Quantum PCA using density matrix diagonalization.

**Algorithm**:
1. Constructs density matrix Ï = (1/N) Î£áµ¢ |xáµ¢âŸ©âŸ¨xáµ¢|
2. Diagonalizes Ï to find eigenvectors
3. Returns top-k eigenvectors as quantum PCA basis

**Key Functions**:
- `quantum_pca(X, k, backend='aer_simulator', shots=8192)`: Quantum PCA simulation
  - Uses Qiskit quantum circuits
  - Simulates measurement outcomes
  - Approximates density matrix eigenvectors
  
- `density_matrix_qpca(X, k)`: Direct density matrix method
  - Faster alternative using classical simulation
  - Mathematically equivalent to quantum approach
  - No quantum circuits needed

**Usage**:
```bash
python quantum/qpca.py \
  --frames data/frame_bank.npy \
  --k 8 \
  --output results/Uq_k8.npz \
  --shots 8192
```

**When to use**: Research, quantum algorithm exploration

---

#### `quantum/project.py`
**Purpose**: Project sequences into PCA subspace.

**Key Functions**:
- `project_sequence(sequence, U)`: Projects single sequence
  - Input: Sequence of shape `(T, 60)`
  - Output: Projected sequence of shape `(T, k)`
  - Formula: X_proj = X @ U

- `project_batch(sequences, U)`: Batch projection for efficiency
- `inverse_project(sequence_proj, U)`: Reconstruct from subspace

**Why it works**: 
- PCA finds directions of maximum variance
- Projecting preserves most important motion patterns
- Lower dimensionality = faster DTW
- Often improves classification (noise reduction)

---

### `dtw/` Directory

#### `dtw/dtw_runner.py`
**Purpose**: DTW distance computation with multiple metrics.

**Key Functions**:
- `dtw_distance(seq1, seq2, metric='euclidean')`: Compute DTW distance
  - Supports: `euclidean`, `cosine`, `fidelity`
  - Returns: Scalar distance value
  - Uses dynamic programming for optimal alignment

- `classify_1nn(test_seq, train_seqs, train_labels, metric='euclidean')`: 1-NN classification
  - Computes DTW to all training sequences
  - Returns label of nearest neighbor

**Distance Metrics**:

1. **Euclidean**: Standard L2 distance
   ```python
   dist = sqrt(sum((a - b)^2))
   ```

2. **Cosine**: Angular distance
   ```python
   dist = 1 - (aÂ·b) / (||a|| ||b||)
   ```

3. **Fidelity**: Quantum state fidelity
   ```python
   dist = 1 - sqrt(sum(sqrt(a * b)))
   ```

**Usage**:
```python
from dtw.dtw_runner import dtw_distance, classify_1nn

# Compute distance
dist = dtw_distance(seq1, seq2, metric='euclidean')

# Classify
pred = classify_1nn(test_seq, train_seqs, train_labels)
```

---

### `eval/` Directory (Evaluation Framework)

#### `eval/aggregate.py`
**Purpose**: Aggregate results from multiple CSV files.

**Key Functions**:
- `load_results(results_dir)`: Load all CSV result files
- `aggregate_metrics(results_dir)`: Compute summary statistics
- `compare_methods(df, metric='accuracy')`: Statistical comparison

**What it does**:
- Reads `metrics_*.csv` files from results directory
- Combines into single DataFrame
- Computes means, stds, min, max
- Enables cross-method comparison

---

#### `eval/plotting.py`
**Purpose**: Plotting utilities for evaluation.

**Key Functions**:
- `plot_accuracy_vs_k(df, output_path)`: Accuracy comparison plot
- `plot_time_vs_k(df, output_path)`: Speed comparison plot  
- `plot_pareto_frontier(df, output_path)`: Pareto frontier analysis
- `set_plot_style()`: Consistent plot styling

**Plot Types**:
1. **Line plots**: Show trends across k values
2. **Bar charts**: Compare discrete configurations
3. **Scatter plots**: Pareto accuracy-time tradeoffs

---

#### `eval/make_figures.py`
**Purpose**: Generate all evaluation figures from results.

**Workflow**:
1. Loads results from CSV files
2. Calls plotting functions for each figure type
3. Saves publication-quality PNGs

**Usage**:
```bash
python eval/make_figures.py \
  --results-dir results \
  --output-dir figures \
  --dpi 300
```

**Output**: 3 figures in `figures/` directory

---

#### `eval/ablations.py`
**Purpose**: Ablation study framework.

**Utility Functions**:
- `add_temporal_jitter(seq, drop_rate, seed)`: Random frame dropping
- `add_joint_noise(seq, sigma, seed)`: Gaussian noise injection
- `sample_frames_uniform(seq, n_samples, seed)`: Uniform temporal sampling
- `sample_frames_energy(seq, n_samples, seed)`: Energy-based sampling

**Experiment Runners**:
- `run_distance_choice_ablation()`: Test distance metrics
- `run_k_sweep_ablation()`: Test dimensionality values
- `run_sampling_strategy_ablation()`: Test frame sampling
- `run_robustness_ablation()`: Test noise/jitter robustness

**Plotting Functions**:
- `plot_distance_choice_ablation()`: Bar chart comparison
- `plot_k_sweep_ablation()`: Line plot of k sweep
- `plot_sampling_strategy_ablation()`: Bar chart comparison
- `plot_robustness_ablation()`: Line plot of robustness

**Why ablations matter**:
- Understand design choice impacts
- Identify optimal configurations
- Test system robustness
- Validate algorithmic decisions

---

### `scripts/` Directory (Pipeline Scripts)

#### `scripts/build_frame_bank.py`
**Purpose**: Extract and organize frames from sequences.

**See**: [Step 1 in Modern Pipeline](#step-1-build-frame-bank)

---

#### `scripts/project_sequences.py`
**Purpose**: Project sequences into PCA subspace.

**See**: [Step 3 in Modern Pipeline](#step-3-project-sequences)

---

#### `scripts/run_dtw_subspace.py`
**Purpose**: Run DTW classification on projected sequences.

**See**: [Step 4 in Modern Pipeline](#step-4-run-dtw-classification)

---

#### `scripts/run_ablations.py`
**Purpose**: Run all ablation experiments.

**See**: [Step 6 in Modern Pipeline](#step-6-run-ablation-studies)

---

#### `scripts/create_label_metadata.py`
**Purpose**: Generate label mappings for projected sequences.

**See**: [Step 7 in Modern Pipeline](#step-7-create-label-metadata-if-needed)

---

### `features/` Directory

#### `features/amplitude_encoding.py`
**Purpose**: Quantum amplitude encoding utilities.

**Key Functions**:
- `encode_unit_vector(x)`: Normalizes single 60-D vector to unit length (L2 norm = 1)
  - Required for quantum amplitude encoding
  - Handles zero vectors gracefully

- `batch_encode_unit_vectors(X)`: Row-wise normalization of multiple vectors
  - Shape: `(T, 60)` â†’ `(T, 60)` with each row normalized

- `verify_normalization(X, tolerance=1e-6)`: Verifies all rows are unit-normalized

**Why Needed**: Quantum states must be normalized (probability amplitudes sum to 1)

---

### `tests/` Directory

#### `tests/test_amplitude_encoding.py`
**Purpose**: Unit tests for amplitude encoding.

**Test Cases**:
- Single vector encoding
- Batch encoding
- Zero vector handling
- Normalization verification
- Edge cases

---

### `scripts/` Directory

#### `scripts/build_frame_bank.py`
**Purpose**: Preprocess and build frame database.

**Workflow**:
1. Loads all skeleton sequences
2. Extracts individual frames
3. Organizes by action class
4. Saves processed frame bank

---

## ğŸ“Š Data Format

### MSR Skeleton Files

**File Naming**: `aXX_sYY_eZZ_skeleton.txt`
- `XX`: Action ID (01-20)
  - 01: High wave
  - 02: Horizontal wave
  - 03: Hammer
  - 08: High throw
  - ... (20 total actions)
- `YY`: Subject ID (01-10)
- `ZZ`: Instance/Execution number (01-03)

**File Content**:
- Each line: 4 values (x, y, z, confidence)
- 20 joints per frame
- Variable number of frames per action

**Joint Structure** (20 joints):
```
Joint 0: Hip Center
Joint 1: Spine
Joint 2: Shoulder Center
Joint 3: Head
Joint 4: Shoulder Left
Joint 5: Elbow Left
Joint 6: Wrist Left
Joint 7: Hand Left
Joint 8: Shoulder Right
Joint 9: Elbow Right
Joint 10: Wrist Right
Joint 11: Hand Right
Joint 12: Hip Left
Joint 13: Knee Left
Joint 14: Ankle Left
Joint 15: Foot Left
Joint 16: Hip Right
Joint 17: Knee Right
Joint 18: Ankle Right
Joint 19: Foot Right
```

**After Processing**:
- Shape: `(num_frames, 20, 3)` â†’ `(num_frames, 60)` (flattened)
- Each frame: 60-dimensional vector (20 joints Ã— 3 coordinates)

---

## ğŸ”§ Installation

### Requirements

```bash
# Core dependencies
pip install numpy scipy matplotlib scikit-learn

# Quantum computing
pip install qiskit qiskit-aer

# GPU acceleration (optional but recommended)
pip install cupy-cuda11x  # Replace 11x with your CUDA version

# Visualization
pip install imageio
```

### Full Installation

```bash
# Clone the repository
git clone <repository-url>
cd qdtw_project

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install numpy scipy matplotlib scikit-learn qiskit qiskit-aer imageio

# Optional: GPU support
pip install cupy-cuda11x
```

---

## ğŸš€ Usage

### 1. Basic Classical Pipeline

Run the classical DTW k-NN classifier:

```bash
cd src
python main.py
```

**Output**: Classification accuracy on test set.

---

### 2. Simple Quantum Demo

Test the basic quantum DTW implementation:

```bash
python qdtw.py
```

**Output**: 
- DTW distance for test sequences
- Quantum minimum search demonstration

---

### 3. Classical vs Quantum Benchmark

Compare classical and quantum approaches:

```bash
python benchmark.py
```

**Output**:
- Classical DTW accuracy and time
- Quantum DTW accuracy and time
- Performance comparison

---

### 4. Comprehensive Quantum Benchmark

Run full quantum algorithm suite:

```bash
python grover_benchmark.py
```

**Output**:
- Multiple algorithm comparisons
- Detailed performance metrics
- Speed and accuracy analysis

**Note**: Requires GPU (CuPy) for optimal performance.

---

### 5. Verify Quantum Circuits

Test quantum circuit correctness:

```bash
python verify_quantum.py
```

**Output**:
- Classical result
- Safe quantum result (with fallback)
- Raw quantum result (no fallback)
- Verification status

---

### 6. Generate Visualizations

Create all project visualizations:

```bash
python generate_all_visuals.py
```

**Output Files** (in `visualizations/`):
- `skeleton_pose.png`
- `skeleton_animation.gif`
- `dtw_alignment.png`
- `grover_circuit.png`
- `quantum_performance.png`
- `accuracy_comparison.png`
- `time_comparison.png`

---

### 7. Amplitude Encoding Demo

Demonstrate quantum encoding:

```bash
python demo_amplitude_encoding.py
```

**Output**:
- Single frame encoding
- Sequence encoding
- Zero vector handling
- Normalization verification

---

## ï¿½ Modern Subspace Projection Pipeline

### Overview

**âš ï¸ CRITICAL WARNING**: This pipeline currently produces **3-5% accuracy** (near-random) due to encoding failure. See "Critical Findings" section at top of README for details. The amplitude encoding + PCA approach destroys class separability.

This is the originally intended pipeline for production use. It uses PCA-based dimensionality reduction to project sequences into a lower-dimensional subspace before applying DTW, with the goal of improving both speed and accuracy.

**Pipeline Steps**:
1. Build frame bank from training data
2. Compute PCA basis (classical or quantum)
3. Project all sequences into k-dimensional subspace
4. Run DTW classification in reduced space
5. Generate evaluation metrics and figures
6. Run ablation studies to analyze design choices

### Step 1: Build Frame Bank

Extract and organize frames from all skeleton sequences:

```bash
python scripts/build_frame_bank.py \
  --data-dir msr_action_data \
  --output data/frame_bank.npy \
  --test-output data/frame_bank_test.npy \
  --test-fraction 0.2 \
  --seed 42
```

**What it does**:
- Loads all MSR Action3D skeleton sequences
- Performs train/test split (80/20 by default)
- Extracts individual frames from each sequence
- Saves frame banks: `frame_bank.npy` (train), `frame_bank_test.npy` (test)

**Output**:
- `data/frame_bank.npy`: Training frames, shape `(N_train_frames, 60)`
- `data/frame_bank_test.npy`: Test frames, shape `(N_test_frames, 60)`

---

### Step 2: Compute PCA Basis

Generate the subspace projection matrix using either classical SVD or quantum PCA:

#### Classical PCA (Recommended - Fast & Accurate)

```bash
python quantum/classical_pca.py \
  --frames data/frame_bank.npy \
  --k 8 \
  --output results/Uc_k8.npz
```

#### Quantum PCA (Research/Educational)

```bash
python quantum/qpca.py \
  --frames data/frame_bank.npy \
  --k 8 \
  --output results/Uq_k8.npz \
  --backend aer_simulator \
  --shots 8192
```

**Arguments**:
- `--frames`: Path to frame bank (from Step 1)
- `--k`: Number of principal components (dimensionality of subspace)
- `--output`: Where to save the PCA basis matrix U
- `--backend`: (qPCA only) Quantum simulator backend
- `--shots`: (qPCA only) Number of quantum circuit shots

**Output**:
- `results/U{c|q}_k{k}.npz`: Contains projection matrix U of shape `(60, k)`
  - `Uc`: Classical PCA basis
  - `Uq`: Quantum PCA basis

**Typical k values**: 3, 5, 8, 10, 12, 16 (8 is a good default)

---

### Step 3: Project Sequences

Project all sequences from 60-D to k-D subspace:

```bash
python scripts/project_sequences.py \
  --data-dir msr_action_data \
  --pca-file results/Uc_k8.npz \
  --output-dir results/subspace/Uc/k8 \
  --test-fraction 0.2 \
  --seed 42
```

**Arguments**:
- `--data-dir`: Directory containing original skeleton files
- `--pca-file`: PCA basis from Step 2 (Uc or Uq)
- `--output-dir`: Where to save projected sequences
- `--test-fraction`: Train/test split ratio
- `--seed`: Random seed for reproducibility

**Output Structure**:
```
results/subspace/Uc/k8/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ seq_0000.npy  # Projected sequence 0, shape (T_0, 8)
â”‚   â”œâ”€â”€ seq_0001.npy  # Projected sequence 1, shape (T_1, 8)
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ metadata.npz  # Labels and filenames
â””â”€â”€ test/
    â”œâ”€â”€ seq_0000.npy
    â”œâ”€â”€ ...
    â””â”€â”€ metadata.npz
```

**Run for multiple configurations**:
```bash
# Classical PCA, multiple k values
for k in 5 8 10; do
  python scripts/project_sequences.py \
    --data-dir msr_action_data \
    --pca-file results/Uc_k${k}.npz \
    --output-dir results/subspace/Uc/k${k}
done

# Quantum PCA, multiple k values
for k in 5 8 10; do
  python scripts/project_sequences.py \
    --data-dir msr_action_data \
    --pca-file results/Uq_k${k}.npz \
    --output-dir results/subspace/Uq/k${k}
done
```

---

### Step 4: Run DTW Classification

**âš ï¸ WARNING**: This step produces random predictions (~5% accuracy) due to encoding failure. The `run_dtw_subspace.py` script also had a bug where it used fake labels (`label = i % 20`). Use `run_ablations.py` instead for corrected label loading.

Perform 1-NN classification using DTW distance on projected sequences:

```bash
python scripts/run_dtw_subspace.py \
  --method Uc \
  --k 8 \
  --metric euclidean \
  --subspace-dir results/subspace
```

**Arguments**:
- `--method`: PCA method (`Uc` for classical, `Uq` for quantum)
- `--k`: Subspace dimensionality
- `--metric`: Distance metric (`euclidean`, `cosine`, or `fidelity`)
- `--subspace-dir`: Base directory containing projected sequences

**Known Issues**:
- Line 84 in `run_dtw_subspace.py` uses `label = i % 20` (fake labels)
- Should load labels from `metadata.npz` files created by `create_label_metadata.py`
- Even with correct labels, accuracy is ~5% due to encoding failure

**Output**:
- `results/metrics_subspace_{method}.csv`: Accuracy and timing for each k value
- Console output: Per-configuration results

**Distance Metrics**:
- `euclidean`: Standard L2 distance
- `cosine`: 1 - cosine_similarity (angular distance)
- `fidelity`: Quantum state fidelity distance (1 - sqrt(fidelity))

**Run all configurations**:
```bash
# Test all methods, k values, and metrics
for method in Uc Uq; do
  for k in 5 8 10; do
    for metric in euclidean cosine fidelity; do
      python scripts/run_dtw_subspace.py \
        --method $method \
        --k $k \
        --metric $metric
    done
  done
done
```

---

### Step 5: Generate Evaluation Figures

Create publication-quality visualizations of results:

```bash
python eval/make_figures.py \
  --results-dir results \
  --output-dir figures \
  --dpi 300
```

**Arguments**:
- `--results-dir`: Directory containing CSV results files
- `--output-dir`: Where to save generated figures
- `--dpi`: Resolution for saved images (default: 300)

**Generated Figures**:

1. **`accuracy_vs_k.png`**: Classification accuracy vs subspace dimensionality
   - Compares Uq and Uc across different k values
   - Shows baseline (60-D) accuracy for reference
   - Helps identify optimal k value

2. **`time_vs_k.png`**: Average computation time vs k
   - Shows time per test sample
   - Illustrates speed/accuracy tradeoff
   - Compares different distance metrics

3. **`pareto_accuracy_time.png`**: Pareto frontier analysis
   - 2D plot: accuracy vs time
   - Identifies optimal configurations
   - Shows dominated vs non-dominated solutions

**Sample Output**:
```
Generated figures:
  - figures/accuracy_vs_k.png (189 KB)
  - figures/time_vs_k.png (374 KB)
  - figures/pareto_accuracy_time.png (233 KB)
```

---

### Step 6: Run Ablation Studies

**âœ… USES CORRECT LABELS**: This script (fixed Nov 6-7, 2025) properly loads labels from `metadata.npz` and reveals the true performance (~3-5% accuracy).

Analyze the impact of design choices and robustness properties:

```bash
python scripts/run_ablations.py \
  --all \
  --n-train 454 \
  --n-test 113 \
  --output-dir results \
  --figures-dir figures
```

**Arguments**:
- `--all`: Run all ablation experiments (or use specific flags below)
- `--distance`: Distance metric comparison (cosine vs euclidean vs fidelity)
- `--k-sweep`: Subspace dimensionality sweep (k âˆˆ {3, 5, 8, 10, 12, 16})
- `--sampling`: Frame sampling strategy (uniform vs energy-based)
- `--robustness`: Noise and temporal jitter robustness
- `--n-train`: Number of training samples to use
- `--n-test`: Number of test samples to use
- `--use-sample-data`: Use synthetic data for testing (optional)

**Current Results** (Nov 7, 2025 - Real Data):
```
Distance Choice (Uq, k=8):
  - cosine:    3.54% accuracy
  - euclidean: 5.31% accuracy  
  - fidelity:  ~5% accuracy

âš ï¸ Near-random performance for 20-class problem (expected 5%)
âš ï¸ Encoding approach has failed
```

**Ablation Experiments**:

1. **Distance Choice** (`--distance`):
   - Compares cosine, euclidean, and fidelity metrics
   - Tests both Uq and Uc methods
   - Identifies best metric for this task

2. **K Sweep** (`--k-sweep`):
   - Tests k âˆˆ {3, 5, 8, 10, 12, 16}
   - Analyzes dimensionality/accuracy tradeoff
   - Finds optimal subspace size

3. **Sampling Strategy** (`--sampling`):
   - **Uniform**: Sample frames at regular intervals
   - **Energy**: Sample high-motion frames (high L2 norm)
   - Determines best frame selection method

4. **Robustness** (`--robustness`):
   - **Noise**: Add Gaussian noise (Ïƒ âˆˆ {0.0, 0.01, 0.02})
   - **Temporal Jitter**: Random frame drops (drop_rate âˆˆ {0.0, 0.05})
   - Tests system robustness to data corruption

**Output Files**:

1. **`results/ablations.csv`**: Raw results for all experiments (REAL PERFORMANCE DATA)
   ```csv
   exp,method,k,metric,setting,accuracy,time_ms
   distance_choice,Uq,,cosine,cosine,0.0354,41213.4
   distance_choice,Uq,,euclidean,euclidean,0.0531,15523.5
   ...
   ```

2. **Generated Figures** (in `figures/`):
   - `ablations_distance.png`: Distance metric comparison (2-panel bar chart)
   - `ablations_k_sweep.png`: Accuracy and time vs k (2-panel line plot)
   - `ablations_sampling.png`: Sampling strategy comparison (2-panel bar chart)
   - `ablations_robustness.png`: Robustness to noise/jitter (2-panel line plot)

**Example Individual Experiments**:
```bash
# Just distance comparison
python scripts/run_ablations.py --distance --n-train 200 --n-test 50

# Just k sweep
python scripts/run_ablations.py --k-sweep --n-train 200 --n-test 50

# Multiple experiments
python scripts/run_ablations.py --distance --k-sweep --robustness
```

---

### Step 7: Create Label Metadata (if needed)

If you need to regenerate label mappings for projected sequences:

```bash
python scripts/create_label_metadata.py
```

**What it does**:
- Parses MSR Action3D filenames (e.g., `a01_s05_e02_skeleton.txt`)
- Extracts action IDs (1-20)
- Creates `metadata.npz` files in each subspace directory
- Maps sequence indices to ground-truth action labels

**When to use**:
- After running `project_sequences.py` for the first time
- If metadata files are missing or corrupted
- To ensure correct labels for evaluation

---

### Complete Pipeline Example

Run the entire pipeline from scratch:

```bash
# 1. Build frame bank
python scripts/build_frame_bank.py \
  --data-dir msr_action_data \
  --output data/frame_bank.npy \
  --test-output data/frame_bank_test.npy

# 2a. Classical PCA (k=8)
python quantum/classical_pca.py \
  --frames data/frame_bank.npy \
  --k 8 \
  --output results/Uc_k8.npz

# 2b. Quantum PCA (k=8)
python quantum/qpca.py \
  --frames data/frame_bank.npy \
  --k 8 \
  --output results/Uq_k8.npz

# 3. Project sequences (both methods)
python scripts/project_sequences.py \
  --data-dir msr_action_data \
  --pca-file results/Uc_k8.npz \
  --output-dir results/subspace/Uc/k8

python scripts/project_sequences.py \
  --data-dir msr_action_data \
  --pca-file results/Uq_k8.npz \
  --output-dir results/subspace/Uq/k8

# 4. Create label metadata
python scripts/create_label_metadata.py

# 5. Run DTW classification
python scripts/run_dtw_subspace.py \
  --method Uc \
  --k 8 \
  --metric euclidean

python scripts/run_dtw_subspace.py \
  --method Uq \
  --k 8 \
  --metric euclidean

# 6. Generate evaluation figures
python eval/make_figures.py

# 7. Run ablation studies
python scripts/run_ablations.py --all
```

**Expected Runtime** (on typical workstation):
- Frame bank: ~1 minute
- Classical PCA: ~5 seconds per k value
- Quantum PCA: ~2-5 minutes per k value
- Projection: ~30 seconds per configuration
- DTW classification: ~5-10 minutes per configuration
- Ablations: ~20-40 minutes for all experiments

---

### Pipeline Output Summary

After running the complete pipeline, you should have:

**Data Files**:
- `data/frame_bank.npy` - Training frames
- `data/frame_bank_test.npy` - Test frames
- `results/Uc_k*.npz` - Classical PCA bases
- `results/Uq_k*.npz` - Quantum PCA bases
- `results/subspace/{Uc|Uq}/k{k}/{train|test}/seq_*.npy` - Projected sequences
- `results/subspace/{Uc|Uq}/k{k}/{train|test}/metadata.npz` - Label mappings

**Results Files**:
- `results/metrics_baseline.csv` - 60-D baseline results
- `results/metrics_subspace_Uc.csv` - Classical PCA results
- `results/metrics_subspace_Uq.csv` - Quantum PCA results
- `results/ablations.csv` - Ablation study results

**Figures**:
- `figures/accuracy_vs_k.png` - Accuracy comparison
- `figures/time_vs_k.png` - Speed comparison
- `figures/pareto_accuracy_time.png` - Pareto frontier
- `figures/ablations_distance.png` - Distance metric analysis
- `figures/ablations_k_sweep.png` - Dimensionality analysis
- `figures/ablations_sampling.png` - Sampling strategy analysis
- `figures/ablations_robustness.png` - Robustness analysis

---

## ï¿½ğŸ”„ Legacy Pipeline Workflow (Original Demo)

### Complete Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. DATA LOADING                          â”‚
â”‚  msr_action_data/*.txt â†’ loader.py â†’ (sequences, labels)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                2. PREPROCESSING                             â”‚
â”‚  Flatten: (T, 20, 3) â†’ (T, 60)                             â”‚
â”‚  Optional: Amplitude encoding (normalize to unit vectors)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               3. TRAIN/TEST SPLIT                           â”‚
â”‚  70% training, 30% testing (sklearn.train_test_split)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              4. DISTANCE COMPUTATION                        â”‚
â”‚  For each test sample:                                      â”‚
â”‚    Classical: dtw_distance(test, train_i)                   â”‚
â”‚    Quantum: qdtw_distance(test, train_i)                    â”‚
â”‚    GPU: dtw_distance_gpu(test, train_i)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          5. MINIMUM SEARCH (k-NN, k=1)                      â”‚
â”‚  Classical: np.argmin(distances)          O(N)              â”‚
â”‚  Grover's: grover_search_minimum()        O(âˆšN)             â”‚
â”‚  QAE: quantum_amplitude_search()          Advanced          â”‚
â”‚  Hybrid: hybrid_quantum_classical()       Adaptive          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                6. CLASSIFICATION                            â”‚
â”‚  Assign label of nearest neighbor                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              7. EVALUATION & RESULTS                        â”‚
â”‚  Accuracy, execution time, speedup metrics                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§® Key Algorithms

### 1. Classical DTW

**Time Complexity**: O(nm) where n, m are sequence lengths

**Algorithm**:
```python
for i in range(1, n+1):
    for j in range(1, m+1):
        cost = distance(seq1[i], seq2[j])
        dtw[i,j] = cost + min(
            dtw[i-1, j],      # Insertion
            dtw[i, j-1],      # Deletion
            dtw[i-1, j-1]     # Match
        )
return dtw[n, m]
```

**Implementation**: `src/dtw.py`

---

### 2. Grover's Quantum Search

**Time Complexity**: O(âˆšN) for searching N items

**Quantum Advantage**: Quadratic speedup over classical O(N) search

**Algorithm Steps**:
1. **Initialize**: Put all qubits in superposition |+âŸ©
2. **Oracle**: Mark the target state (minimum value index)
3. **Diffusion**: Amplify amplitude of marked state
4. **Repeat**: Ï€/4 Ã— âˆšN iterations
5. **Measure**: Obtain result with high probability

**Key Components**:
```python
# Oracle (marks minimum index)
for each qubit:
    if bit == 0: apply X gate
apply multi-controlled Z gate
uncompute X gates

# Diffusion Operator
apply H to all qubits
apply X to all qubits
apply multi-controlled Z
uncompute X gates
apply H to all qubits
```

**Implementation**: `quantum_src/classifier.py`

---

### 3. Quantum Amplitude Estimation (QAE)

**Purpose**: Estimate amplitude of quantum state more efficiently

**Use Case**: Finding minimum by encoding distances as amplitudes

**Algorithm**:
1. Encode distances as quantum amplitudes
2. Apply phase rotations based on values
3. Use entanglement to correlate states
4. Measure and extract minimum

**Advantage**: Can provide better precision than Grover's for certain problems

**Implementation**: `quantum_src/quantum_amp_est.py`

---

### 4. Hybrid Quantum-Classical

**Strategy**: Use quantum only when beneficial

**Decision Logic**:
```python
if N < threshold:
    return classical_search()  # Faster for small N
elif close_values < 3:
    return classical_search()  # Clear winner
elif close_values > N/3:
    return classical_search()  # Too noisy for quantum
else:
    return quantum_search()    # Sweet spot for quantum
```

**Implementation**: `quantum_src/quantum_amp_est.py`

---

## ğŸ“ˆ Performance Characteristics

### Expected Results

| Algorithm | Complexity | Accuracy | Speed (relative) |
|-----------|-----------|----------|------------------|
| Classical CPU DTW | O(N) | Baseline | 1.0x |
| Classical GPU DTW | O(N) | Same | 2-5x faster |
| Quantum DTW (simple) | O(âˆšN) | Same | Depends on N |
| Grover's Search | O(âˆšN) | Same | Faster for N>100 |
| QAE | O(log N) | Same | Faster for large N |
| Hybrid | Adaptive | Same | Best overall |

**Note**: Quantum speedup depends on:
- Problem size (N)
- Quantum hardware quality
- Overhead of classical-quantum interface

---

## ğŸ” Troubleshooting

### Common Issues

1. **Missing Dependencies**
   ```bash
   pip install qiskit qiskit-aer numpy scipy scikit-learn
   ```

2. **GPU Not Available**
   - GPU code (CuPy) is optional
   - Project works on CPU only (slower)

3. **Data Files Not Found**
   - Ensure `msr_action_data/` directory exists
   - Check skeleton files are present (`.txt` format)

4. **Quantum Circuit Errors**
   - Increase shots for better accuracy
   - Check Qiskit version compatibility

5. **Memory Issues**
   - Reduce test set size
   - Use batch processing
   - Close unused applications

---

## ğŸ“š References

### Papers & Theory

- **Dynamic Time Warping**: Sakoe & Chiba (1978)
- **Grover's Algorithm**: Grover (1996)
- **Quantum Amplitude Estimation**: Brassard et al. (2002)
- **MSR Action3D Dataset**: Li et al. (2010)

### Libraries Used

- **Qiskit**: Quantum computing framework
- **NumPy**: Numerical computing
- **SciPy**: Scientific computing
- **Scikit-learn**: Machine learning
- **CuPy**: GPU-accelerated NumPy
- **Matplotlib**: Visualization

---

## ğŸ“ Understanding the Code

### For Beginners

1. **Start with classical implementation**:
   - Read `src/loader.py` to understand data format
   - Study `src/dtw.py` for DTW algorithm
   - Check `src/main.py` for complete pipeline

2. **Learn quantum concepts**:
   - Run `qdtw.py` to see basic quantum search
   - Read `verify_quantum.py` to understand circuit verification
   - Study `quantum_src/classifier.py` for Grover's implementation

3. **Explore benchmarking**:
   - Run `benchmark.py` for simple comparison
   - Try `grover_benchmark.py` for comprehensive analysis

### For Advanced Users

- Modify quantum circuits in `quantum_src/`
- Implement custom distance metrics
- Experiment with different quantum algorithms
- Optimize GPU kernels in GPU files
- Add new visualization types

---

## ğŸ“ Notes

- **Quantum Simulator**: Uses Qiskit Aer simulator (not real quantum hardware)
- **Scalability**: Currently optimized for datasets with 100-1000 samples
- **Accuracy**: Quantum and classical methods should have identical accuracy (same DTW logic)
- **Speed**: Quantum speedup is theoretical; simulator overhead may dominate for small problems

---

## ğŸ¤ Contributing


To extend this project:
1. **FIX THE ENCODING FIRST** - Test classical DTW on raw 60-D features without encoding
2. Investigate alternative quantum encoding schemes that preserve discriminative information
3. Try higher dimensionality (k=20, 30, 40) to see if more dimensions help
4. Validate that PCA bases capture action-specific patterns (not just variance)
5. Add new quantum algorithms in `quantum_src/`
6. Implement additional classical baselines
7. Add more visualization types in `generate_all_visuals.py`
8. Improve encoding schemes in `features/`
9. Add unit tests in `tests/`

---

## ğŸš¨ Known Issues & Limitations

### Critical Issues (Nov 7, 2025)

1. **Encoding Failure**: Amplitude encoding + PCA destroys class separability
   - Current accuracy: 3-5% (near-random)
   - Expected baseline: 60-80%
   - Root cause: Normalized frames lose discriminative structure
   
2. **Fake Metrics Files**: Previous "results" were synthetic data
   - `metrics_subspace_Uc.csv` and `metrics_subspace_Uq.csv` generated by `create_sample_metrics()`
   - Documented "82.99% accuracy" never actually achieved
   - Real performance revealed by ablation studies (Nov 7, 2025)

3. **Label Loading Bug** (FIXED Nov 6-7, 2025):
   - `scripts/run_dtw_subspace.py` line 84: Used `label = i % 20` (fake labels)
   - `scripts/run_ablations.py`: Now correctly loads from `metadata.npz`

### Recommendations

**Before using this project**:
1. âš ï¸ Do NOT trust the documented 82.99% accuracy - it was fake
2. âš ï¸ Current quantum encoding approach does NOT work
3. âœ… Use this as a framework/reference implementation only
4. âœ… Test classical baseline first (raw features, no encoding)

**To fix the pipeline**:
1. Run classical DTW on original 60-D skeleton features (no PCA, no encoding)
2. Compare to encoded+projected results to quantify information loss
3. Investigate why encoding destroys class structure
4. Try alternative quantum feature maps or skip encoding entirely

---

## ğŸ“§ Contact & Support

For questions or issues:
1. Check this README thoroughly
2. Review code comments in relevant files
3. Test with simple examples first
4. Verify dependencies are installed correctly
5. **READ THE CRITICAL FINDINGS SECTION** at the top of this file

---

## ğŸ“š Additional Documentation

For more detailed information, see:

- **`docs/PIPELINE_GUIDE.md`**: Technical deep-dive into the complete pipeline
- **`docs/DEBUGGING_REPORT.md`**: Root cause analysis of the L2 normalization bug
- **`docs/SOLUTION_SUMMARY.md`**: Summary of findings and solution
- **`docs/archive/`**: Historical documentation from debugging process

---

**Last Updated**: November 12, 2025

**Project Status**: âœ… **FIXED** - Standardization-based pipeline achieves **72-74% accuracy**

**Previous Status**: âŒ L2 normalization produced 3-5% accuracy (Nov 7-11, 2025)

**Key Achievement**: 20-24Ã— accuracy improvement through proper feature standardization

**Current Status**: âœ… Production-ready with documented, reproducible results

---

## ğŸ“„ License

[Add your license here]

